{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ee9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703cf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f65cf5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce218480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "blanks = []\n",
    "for i, rv, sn in df.itertuples():\n",
    "    if type(rv) == str and rv.isspace():\n",
    "        blanks.append(i)\n",
    "        \n",
    "if len(blanks) > 0:\n",
    "    print(f'There are {len(blanks)} empty space strings in the dataset')\n",
    "    df.drop(blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa1942fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       49582\n",
       "sentiment        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285ce8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd79aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 13704\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = df['review'].str.len().max()\n",
    "\n",
    "print(\"Max sequence length:\", max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da900a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 390931\n"
     ]
    }
   ],
   "source": [
    "words = set(word.lower() for sentence in df['review'] for word in sentence.split())\n",
    "n_unique_words = len(words)\n",
    "\n",
    "print(\"Unique words:\", n_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a35054a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. <br /><br />The...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        positive\n",
       "1        positive\n",
       "2        positive\n",
       "3        negative\n",
       "4        positive\n",
       "           ...   \n",
       "49995    positive\n",
       "49996    negative\n",
       "49997    negative\n",
       "49998    negative\n",
       "49999    negative\n",
       "Name: sentiment, Length: 50000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "display(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba97a8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dfa8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "max_len = 150\n",
    "num_words = 1000\n",
    "\n",
    "tok = Tokenizer(num_words=num_words)\n",
    "tok.fit_on_texts(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a141d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.3, random_state=101)\n",
    "\n",
    "X_train_mat = tok.texts_to_sequences(X_train)\n",
    "X_test_mat = tok.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_mat, maxlen=max_len)\n",
    "X_test_padded = pad_sequences(X_test_mat, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e5d411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4411e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "def create_gru_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(GRU(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b068129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout_rate = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80c0dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = create_gru_model(units, dropout_rate)\n",
    "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03fdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# Train the models and measure time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1c2aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 365s 2s/step - loss: 0.4760 - accuracy: 0.7580 - val_loss: 0.3689 - val_accuracy: 0.8371\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 238s 1s/step - loss: 0.3381 - accuracy: 0.8548 - val_loss: 0.3444 - val_accuracy: 0.8514\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 235s 1s/step - loss: 0.3168 - accuracy: 0.8678 - val_loss: 0.3403 - val_accuracy: 0.8511\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 240s 1s/step - loss: 0.3006 - accuracy: 0.8751 - val_loss: 0.3313 - val_accuracy: 0.8630\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 238s 1s/step - loss: 0.2890 - accuracy: 0.8802 - val_loss: 0.3235 - val_accuracy: 0.8684\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 238s 1s/step - loss: 0.2724 - accuracy: 0.8896 - val_loss: 0.3182 - val_accuracy: 0.8690\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 237s 1s/step - loss: 0.2584 - accuracy: 0.8924 - val_loss: 0.3385 - val_accuracy: 0.8670\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 236s 1s/step - loss: 0.2501 - accuracy: 0.8973 - val_loss: 0.3273 - val_accuracy: 0.8660\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 239s 1s/step - loss: 0.2405 - accuracy: 0.9019 - val_loss: 0.3269 - val_accuracy: 0.8656\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "gru_history = gru_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59355790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming you have trained and obtained a model named model_bi_lstm\n",
    "\n",
    "# Save the trained model to an HDF5 file\n",
    "gru_model.save('gru_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc14dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 94s 798ms/step\n",
      "Correct Predictions: 12990\n",
      "Wrong Predictions: 2010\n",
      "Accuracy: 86.60%\n"
     ]
    }
   ],
   "source": [
    "# Predict using the trained model on test data\n",
    "y_pred_prob = gru_model.predict(X_test_padded, batch_size=128)\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for true_label, pred_label in zip(y_test, y_pred) if true_label == pred_label)\n",
    "accuracy = correct_predictions / len(y_pred) * 100\n",
    "\n",
    "print('Correct Predictions:', correct_predictions)\n",
    "print('Wrong Predictions:', len(y_pred) - correct_predictions)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca046103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = load_model('gru_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3a41b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d55d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  One of the other reviewers has mentioned that after watching just  Oz episode youll be hooked They are right as this is exactly what happened with mebr br The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the wordbr br It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to manyAryans Muslims gangstas Latinos Christians Italians Irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\n",
      "Filtered:  ['one reviewers mentioned watching  oz episode youll hooked they right exactly happened mebr br the first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid this show pulls punches regards drugs sex violence its hardcore classic use wordbr br it called oz nickname given oswald maximum security state penitentary it focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br i would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around the first episode i ever saw struck nasty surreal i couldnt say i ready i watched i developed taste oz got accustomed high levels graphic violence not violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords  # Import NLTK's stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Download the stopwords data if not already downloaded\n",
    "\n",
    "#regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82777808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b181e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476cf80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3982489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "804ac645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered_text: one reviewers mentioned watching  oz episode youll hooked right exactly happened mebr br first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n",
      "[[ 19  20  21   7   1   8  22  23   9  24  25  26   3  10  27  11   1  28\n",
      "   29  30   2  31   9  32  33  34   4  35  36  37   4  38  39  40  41  42\n",
      "    2  43  44  45  46   3  47   1  48  49  50  51  52  53  54  55  56  57\n",
      "   12  58  59   5  60  61  62  63  64  65  13  66  67  12  68  69  70  71\n",
      "   72  73  74  75  76  77  78  79  80  81  82  83  84  85  86   3  87  14\n",
      "   88  89   4  15  90  91  92  93  94   6  95  96  97  98  99   6 100   6\n",
      "  101 102 103 104  10   8 105 106  11 107 108 109  14 110 111 112 113   1\n",
      "  114 115  13 116 117   2   2 118 119 120  16 121 122  17  16 123 124  18\n",
      "  125 126 127 128 129  17]]\n",
      "1/1 [==============================] - 1s 680ms/step\n",
      "[[0.8368327]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords data if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define your text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    text = regex.sub('', text)\n",
    "    words = text.split(' ')\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    filtered = [w.lower() for w in words if w.lower() not in english_stops]\n",
    "    filtered_text = ' '.join(filtered)\n",
    "    return filtered_text\n",
    "\n",
    "# Example text\n",
    "review = \"One of the other reviewers has mentioned that after watching just  Oz episode youll be hooked They are right as this is exactly what happened with mebr br The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the wordbr br It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to manyAryans Muslims gangstas Latinos Christians Italians Irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\"\n",
    "\n",
    "# Preprocess the text\n",
    "filtered_text = preprocess_text(review)\n",
    "print('Filtered_text:',filtered_text)\n",
    "\n",
    "# Define the maximum length for padding\n",
    "max_length = 150  # Adjust as per your model's input shape\n",
    "\n",
    "# Create and fit a Tokenizer object\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([filtered_text])\n",
    "\n",
    "# Tokenize the words and pad sequences\n",
    "tokenize_words = token.texts_to_sequences([filtered_text])\n",
    "tokenize_words_padded = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(tokenize_words_padded)\n",
    "\n",
    "# Assuming loaded_model is your pre-trained model\n",
    "result = loaded_model.predict(tokenize_words_padded)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "434fb9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: one reviewers mentioned watching  oz episode youll hooked right exactly happened mebr br first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n",
      "The Review is positive\n"
     ]
    }
   ],
   "source": [
    "print('Text:',filtered_text)\n",
    "if result >= 0.7:\n",
    "    print('The Review is positive')\n",
    "else:\n",
    "    print('The Review is negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6810f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 655ms/step\n",
      "[[0.60752356]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Download NLTK stopwords data if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define your text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    text = regex.sub('', text)\n",
    "    words = text.split(' ')\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    filtered = [w.lower() for w in words if w.lower() not in english_stops]\n",
    "    filtered_text = ' '.join(filtered)\n",
    "    return filtered_text\n",
    "\n",
    "# Example text\n",
    "review = \"special effects, just simply good acting and getting simple things right,and MOst importantly--not being LAME--, but i guess this was produced for those Sheeple without taste and not a clue of what is ''A good Movie''don't be scared of rating films low,save your under-appreciated high scores for ''once in a life time movies''. Keep in mind that many use IMDb for trusted reviews and opinions,don't spoil the broth by sugarcoating turds Peace & love\"\n",
    "# Preprocess the text\n",
    "filtered_text = preprocess_text(review)\n",
    "\n",
    "# Define the maximum length for padding\n",
    "max_length = 150  # Adjust as per your model's input shape\n",
    "\n",
    "# Create and fit a Tokenizer object\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([filtered_text])\n",
    "\n",
    "# Tokenize the words and pad sequences\n",
    "tokenize_words = token.texts_to_sequences([filtered_text])\n",
    "tokenize_words_padded = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Load the pre-trained GRU model\n",
    "loaded_model = load_model('gru_model.h5')\n",
    "\n",
    "# Make predictions\n",
    "result = loaded_model.predict(tokenize_words_padded)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c9859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: special effects simply good acting getting simple things rightand importantlynot lame guess produced sheeple without taste clue good moviedont scared rating films lowsave underappreciated high scores life time movies keep mind many use imdb trusted reviews opinionsdont spoil broth sugarcoating turds peace  love\n",
      "\u001b[1mThe Review is negative\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('Text:', filtered_text)\n",
    "if result >= 0.7:\n",
    "    print('\\033[1mThe Review is positive\\033[0m')  # Highlight positive review\n",
    "else:\n",
    "    print('\\033[1mThe Review is negative\\033[0m')  # Highlight negative review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faac2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RNN\n",
    "def create_rnn_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(SimpleRNN(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# LSTM\n",
    "def create_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# GRU\n",
    "def create_gru_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(GRU(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# CNN\n",
    "def create_cnn_model(filters, kernel_size, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Bidirectional LSTM\n",
    "def create_bidirectional_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(units)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Bidirectional GRU\n",
    "def create_bidirectional_gru_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, input_length=max_len))\n",
    "    model.add(Bidirectional(GRU(units)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6b82667",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout_rate = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc4de16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "rnn_model = create_rnn_model(units, dropout_rate)\n",
    "lstm_model = create_lstm_model(units, dropout_rate)\n",
    "gru_model = create_gru_model(units, dropout_rate)\n",
    "cnn_model = create_cnn_model(filters=64, kernel_size=3, dropout_rate=0.5)\n",
    "bidirectional_lstm_model = create_bidirectional_lstm_model(units, dropout_rate)\n",
    "bidirectional_gru_model = create_bidirectional_gru_model(units, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fec2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile models\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "bidirectional_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "bidirectional_gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11ca9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# Train the models and measure time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dbbd892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 112s 511ms/step - loss: 0.5608 - accuracy: 0.7140 - val_loss: 0.4530 - val_accuracy: 0.7997\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 110s 505ms/step - loss: 0.4022 - accuracy: 0.8273 - val_loss: 0.3720 - val_accuracy: 0.8410\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 131s 598ms/step - loss: 0.3652 - accuracy: 0.8474 - val_loss: 0.3794 - val_accuracy: 0.8429\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 221s 1s/step - loss: 0.3393 - accuracy: 0.8598 - val_loss: 0.3846 - val_accuracy: 0.8359\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 221s 1s/step - loss: 0.3154 - accuracy: 0.8715 - val_loss: 0.3812 - val_accuracy: 0.8444\n",
      "Epoch 1/10\n",
      "219/219 [==============================] - 408s 2s/step - loss: 0.4673 - accuracy: 0.7759 - val_loss: 0.3459 - val_accuracy: 0.8486\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 447s 2s/step - loss: 0.3320 - accuracy: 0.8601 - val_loss: 0.3568 - val_accuracy: 0.8417\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 399s 2s/step - loss: 0.3213 - accuracy: 0.8650 - val_loss: 0.3394 - val_accuracy: 0.8554\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 395s 2s/step - loss: 0.3014 - accuracy: 0.8726 - val_loss: 0.3337 - val_accuracy: 0.8556\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 329s 1s/step - loss: 0.2906 - accuracy: 0.8782 - val_loss: 0.3520 - val_accuracy: 0.8524\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 397s 2s/step - loss: 0.2758 - accuracy: 0.8855 - val_loss: 0.3353 - val_accuracy: 0.8611\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 395s 2s/step - loss: 0.2723 - accuracy: 0.8848 - val_loss: 0.3460 - val_accuracy: 0.8490\n",
      "Epoch 1/10\n",
      "219/219 [==============================] - 25s 115ms/step - loss: 0.5623 - accuracy: 0.7094 - val_loss: 0.4088 - val_accuracy: 0.8167\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 26s 117ms/step - loss: 0.4046 - accuracy: 0.8190 - val_loss: 0.3650 - val_accuracy: 0.8400\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 25s 114ms/step - loss: 0.3619 - accuracy: 0.8438 - val_loss: 0.3466 - val_accuracy: 0.8444\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 37s 170ms/step - loss: 0.3301 - accuracy: 0.8585 - val_loss: 0.3365 - val_accuracy: 0.8521\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 38s 175ms/step - loss: 0.3138 - accuracy: 0.8667 - val_loss: 0.3300 - val_accuracy: 0.8524\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 33s 149ms/step - loss: 0.2924 - accuracy: 0.8777 - val_loss: 0.3246 - val_accuracy: 0.8580\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 23s 104ms/step - loss: 0.2772 - accuracy: 0.8863 - val_loss: 0.3236 - val_accuracy: 0.8563\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 23s 105ms/step - loss: 0.2608 - accuracy: 0.8935 - val_loss: 0.3249 - val_accuracy: 0.8550\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 23s 104ms/step - loss: 0.2481 - accuracy: 0.9014 - val_loss: 0.3250 - val_accuracy: 0.8576\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 23s 107ms/step - loss: 0.2376 - accuracy: 0.9043 - val_loss: 0.3270 - val_accuracy: 0.8573\n",
      "Epoch 1/10\n",
      "219/219 [==============================] - 427s 2s/step - loss: 0.4477 - accuracy: 0.7862 - val_loss: 0.3560 - val_accuracy: 0.8410\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 424s 2s/step - loss: 0.3401 - accuracy: 0.8572 - val_loss: 0.3441 - val_accuracy: 0.8506\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 445s 2s/step - loss: 0.3173 - accuracy: 0.8679 - val_loss: 0.3451 - val_accuracy: 0.8503\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 423s 2s/step - loss: 0.3054 - accuracy: 0.8733 - val_loss: 0.3361 - val_accuracy: 0.8549\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 428s 2s/step - loss: 0.2947 - accuracy: 0.8786 - val_loss: 0.3716 - val_accuracy: 0.8406\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 441s 2s/step - loss: 0.2831 - accuracy: 0.8811 - val_loss: 0.3349 - val_accuracy: 0.8536\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 427s 2s/step - loss: 0.2695 - accuracy: 0.8869 - val_loss: 0.3451 - val_accuracy: 0.8587\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 557s 3s/step - loss: 0.2623 - accuracy: 0.8916 - val_loss: 0.3356 - val_accuracy: 0.8524\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 870s 4s/step - loss: 0.2484 - accuracy: 0.8964 - val_loss: 0.3373 - val_accuracy: 0.8551\n",
      "Epoch 1/10\n",
      "219/219 [==============================] - 954s 4s/step - loss: 0.4827 - accuracy: 0.7536 - val_loss: 0.3660 - val_accuracy: 0.8420\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 999s 5s/step - loss: 0.3409 - accuracy: 0.8564 - val_loss: 0.3385 - val_accuracy: 0.8550\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 1020s 5s/step - loss: 0.3198 - accuracy: 0.8660 - val_loss: 0.3331 - val_accuracy: 0.8567\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 539s 2s/step - loss: 0.3000 - accuracy: 0.8763 - val_loss: 0.3298 - val_accuracy: 0.8563\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 584s 3s/step - loss: 0.2853 - accuracy: 0.8824 - val_loss: 0.3313 - val_accuracy: 0.8651\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 634s 3s/step - loss: 0.2665 - accuracy: 0.8905 - val_loss: 0.3318 - val_accuracy: 0.8631\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 691s 3s/step - loss: 0.2547 - accuracy: 0.8956 - val_loss: 0.3277 - val_accuracy: 0.8664\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 1128s 5s/step - loss: 0.2409 - accuracy: 0.9021 - val_loss: 0.3621 - val_accuracy: 0.8639\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 1064s 5s/step - loss: 0.2242 - accuracy: 0.9085 - val_loss: 0.3537 - val_accuracy: 0.8601\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 1097s 5s/step - loss: 0.2165 - accuracy: 0.9134 - val_loss: 0.3607 - val_accuracy: 0.8527\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "rnn_history = rnn_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "# LSTM\n",
    "lstm_history = lstm_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "# CNN\n",
    "cnn_history = cnn_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Bidirectional LSTM\n",
    "bidirectional_lstm_history = bidirectional_lstm_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Bidirectional GRU\n",
    "bidirectional_gru_history = bidirectional_gru_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46a0b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7d83575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 252s 537ms/step - loss: 0.3792 - accuracy: 0.8379\n",
      "469/469 [==============================] - 221s 471ms/step - loss: 0.3287 - accuracy: 0.8560\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.3210 - accuracy: 0.8595\n",
      "469/469 [==============================] - 292s 623ms/step - loss: 0.3335 - accuracy: 0.8563\n",
      "469/469 [==============================] - 326s 695ms/step - loss: 0.3270 - accuracy: 0.8647\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models on the test set\n",
    "rnn_eval = rnn_model.evaluate(X_test_padded, y_test)\n",
    "lstm_eval = lstm_model.evaluate(X_test_padded, y_test)\n",
    "\n",
    "cnn_eval = cnn_model.evaluate(X_test_padded, y_test)\n",
    "bidirectional_lstm_eval = bidirectional_lstm_model.evaluate(X_test_padded, y_test)\n",
    "bidirectional_gru_eval = bidirectional_gru_model.evaluate(X_test_padded, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047346fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performance metrics\n",
    "rnn_test_acc = rnn_eval[1]\n",
    "lstm_test_acc = lstm_eval[1]\n",
    "\n",
    "cnn_test_acc = cnn_eval[1]\n",
    "bidirectional_lstm_test_acc = bidirectional_lstm_eval[1]\n",
    "bidirectional_gru_test_acc = bidirectional_gru_eval[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance metrics and training time\n",
    "print(\"RNN Test Accuracy:\", rnn_test_acc)\n",
    "print(\"LSTM Test Accuracy:\", lstm_test_acc)\n",
    "\n",
    "print(\"CNN Test Accuracy:\", cnn_test_acc)\n",
    "print(\"Bidirectional LSTM Test Accuracy:\", bidirectional_lstm_test_acc)\n",
    "print(\"Bidirectional GRU Test Accuracy:\", bidirectional_gru_test_acc)\n",
    "print(\"Training Time (seconds):\", training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3c6c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming you have trained and obtained a model named model_bi_lstm\n",
    "\n",
    "# Save the trained model to an HDF5 file\n",
    "rnn_model.save('imdbmodel/rnn_model.h5')\n",
    "cnn_model.save('imdbmodel/cnn_model.h5')\n",
    "lstm_model.save('imdbmodel/lstm_model.h5')\n",
    "bidirectional_lstm_model.save('imdbmodel/bidirectional_lstm_model.h5')\n",
    "bidirectional_gru_model.save('imdbmodel/bidirectional_gru_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f771ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bf54d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 611ms/step\n",
      "[[0.60752356]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Download NLTK stopwords data if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define your text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    text = regex.sub('', text)\n",
    "    words = text.split(' ')\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    filtered = [w.lower() for w in words if w.lower() not in english_stops]\n",
    "    filtered_text = ' '.join(filtered)\n",
    "    return filtered_text\n",
    "\n",
    "# Example text\n",
    "review = \"special effects, just simply good acting and getting simple things right,and MOst importantly--not being LAME--, but i guess this was produced for those Sheeple without taste and not a clue of what is ''A good Movie''don't be scared of rating films low,save your under-appreciated high scores for ''once in a life time movies''. Keep in mind that many use IMDb for trusted reviews and opinions,don't spoil the broth by sugarcoating turds Peace & love\"\n",
    "# Preprocess the text\n",
    "filtered_text = preprocess_text(review)\n",
    "\n",
    "# Define the maximum length for padding\n",
    "max_length = 150  # Adjust as per your model's input shape\n",
    "\n",
    "# Create and fit a Tokenizer object\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([filtered_text])\n",
    "\n",
    "# Tokenize the words and pad sequences\n",
    "tokenize_words = token.texts_to_sequences([filtered_text])\n",
    "tokenize_words_padded = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Load the pre-trained GRU model\n",
    "loaded_model = load_model('gru_model.h5')\n",
    "\n",
    "# Make predictions\n",
    "result = loaded_model.predict(tokenize_words_padded)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d8861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: special effects simply good acting getting simple things rightand importantlynot lame guess produced sheeple without taste clue good moviedont scared rating films lowsave underappreciated high scores life time movies keep mind many use imdb trusted reviews opinionsdont spoil broth sugarcoating turds peace  love\n",
      "\u001b[1mThe Review is negative\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('Text:', filtered_text)\n",
    "if result >= 0.7:\n",
    "    print('\\033[1mThe Review is positive\\033[0m')  # Highlight positive review\n",
    "else:\n",
    "    print('\\033[1mThe Review is negative\\033[0m')  # Highlight negative review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08bee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered_text: one reviewers mentioned watching  oz episode youll hooked right exactly happened mebr br first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n",
      "[[ 19  20  21   7   1   8  22  23   9  24  25  26   3  10  27  11   1  28\n",
      "   29  30   2  31   9  32  33  34   4  35  36  37   4  38  39  40  41  42\n",
      "    2  43  44  45  46   3  47   1  48  49  50  51  52  53  54  55  56  57\n",
      "   12  58  59   5  60  61  62  63  64  65  13  66  67  12  68  69  70  71\n",
      "   72  73  74  75  76  77  78  79  80  81  82  83  84  85  86   3  87  14\n",
      "   88  89   4  15  90  91  92  93  94   6  95  96  97  98  99   6 100   6\n",
      "  101 102 103 104  10   8 105 106  11 107 108 109  14 110 111 112 113   1\n",
      "  114 115  13 116 117   2   2 118 119 120  16 121 122  17  16 123 124  18\n",
      "  125 126 127 128 129  17]]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[[0.8368327]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords data if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define your text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    text = regex.sub('', text)\n",
    "    words = text.split(' ')\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    filtered = [w.lower() for w in words if w.lower() not in english_stops]\n",
    "    filtered_text = ' '.join(filtered)\n",
    "    return filtered_text\n",
    "\n",
    "# Example text\n",
    "review = \"One of the other reviewers has mentioned that after watching just  Oz episode youll be hooked They are right as this is exactly what happened with mebr br The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the wordbr br It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to manyAryans Muslims gangstas Latinos Christians Italians Irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\"\n",
    "\n",
    "# Preprocess the text\n",
    "filtered_text = preprocess_text(review)\n",
    "print('Filtered_text:',filtered_text)\n",
    "\n",
    "# Define the maximum length for padding\n",
    "max_length = 150  # Adjust as per your model's input shape\n",
    "\n",
    "# Create and fit a Tokenizer object\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([filtered_text])\n",
    "\n",
    "# Tokenize the words and pad sequences\n",
    "tokenize_words = token.texts_to_sequences([filtered_text])\n",
    "tokenize_words_padded = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(tokenize_words_padded)\n",
    "\n",
    "# Assuming loaded_model is your pre-trained model\n",
    "result = loaded_model.predict(tokenize_words_padded)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15a9526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: one reviewers mentioned watching  oz episode youll hooked right exactly happened mebr br first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n",
      "\u001b[1mThe Review is positive\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('Text:', filtered_text)\n",
    "if result >= 0.7:\n",
    "    print('\\033[1mThe Review is positive\\033[0m')  # Highlight positive review\n",
    "else:\n",
    "    print('\\033[1mThe Review is negative\\033[0m')  # Highlight negative review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5a7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
